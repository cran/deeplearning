% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/finetune_SGD.R
\name{finetune_SGD_bn}
\alias{finetune_SGD_bn}
\title{Updates a deep neural network's parameters using stochastic gradient descent
 method and batch normalization}
\usage{
finetune_SGD_bn(darch, trainData, targetData, learn_rate_weight = exp(-10),
  learn_rate_bias = exp(-10), learn_rate_gamma = exp(-10),
  errorFunc = meanSquareErr, with_BN = T)
}
\arguments{
\item{darch}{a darch instance}

\item{trainData}{training input}

\item{targetData}{training target}

\item{learn_rate_weight}{leanring rate for the weight matrices}

\item{learn_rate_bias}{learning rate for the biases}

\item{learn_rate_gamma}{learning rate for the gammas}

\item{errorFunc}{the error function to minimize during training}

\item{with_BN}{logical value, T to train the neural net with batch normalization}
}
\value{
a darch instance with parameters updated with stochastic gradient descent
}
\description{
This function finetunes a DArch network using SGD approach
}

